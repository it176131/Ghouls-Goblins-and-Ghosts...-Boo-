{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# analysis\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# visuals\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns; sns.set()\n",
    "\n",
    "# preprocessing\n",
    "from sklearn.preprocessing import MinMaxScaler, RobustScaler, PolynomialFeatures\n",
    "\n",
    "# feature selection\n",
    "from sklearn.feature_selection import VarianceThreshold, SelectPercentile\n",
    "\n",
    "# decomposition\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# ensemble\n",
    "from sklearn.ensemble import AdaBoostClassifier, ExtraTreesClassifier, RandomForestClassifier\n",
    "\n",
    "# logistic regression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# naive bayes\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "# process classifier\n",
    "from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "\n",
    "# neighbors\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# neural networks\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "# support vector machines\n",
    "from sklearn.svm import LinearSVC, SVC\n",
    "\n",
    "# train test split\n",
    "from sklearn.model_selection import train_test_split, cross_validate\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifiers = {\n",
    "    'AdaBoostClassifier' : AdaBoostClassifier(random_state=0),\n",
    "    'ExtraTreesClassifier' : ExtraTreesClassifier(n_estimators=100, random_state=0),\n",
    "    'RandomForestClassifier' : RandomForestClassifier(n_estimators=100, random_state=0),\n",
    "    'LogisticRegression' : LogisticRegression(solver='lbfgs', multi_class='auto', random_state=0, max_iter=200),\n",
    "    'GaussianNB' : GaussianNB(),\n",
    "    'GaussianProcessClassifier' : GaussianProcessClassifier(random_state=0),\n",
    "    'KNeighborsClassifier' : KNeighborsClassifier(),\n",
    "    'MLPClassifier' : MLPClassifier(random_state=0, max_iter=300),\n",
    "    'LinearSVC' : LinearSVC(random_state=0, max_iter=1100),\n",
    "    'SVC' : SVC(gamma='scale', random_state=0)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensembles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ensembles = {\n",
    "    'AdaBoostClassifier' : AdaBoostClassifier(random_state=0),\n",
    "    'ExtraTreesClassifier' : ExtraTreesClassifier(n_estimators=100, random_state=0),\n",
    "    'RandomForestClassifier' : RandomForestClassifier(n_estimators=100, random_state=0)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read training data\n",
    "train = pd.read_csv(\"train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Categorical Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.describe(include=['O'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to predict the `type` field."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.color.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get dummy color features\n",
    "train = train.join(pd.get_dummies(train.color, prefix='color'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select X features\n",
    "X = train.drop(columns=[\n",
    "    'id',\n",
    "    'color',\n",
    "    'type',\n",
    "])\n",
    "\n",
    "# select y for training\n",
    "y = train.type\n",
    "\n",
    "# define scaler\n",
    "scaler = MinMaxScaler()\n",
    "rscaler = RobustScaler()\n",
    "\n",
    "# scale X\n",
    "X_ = scaler.fit_transform(X)\n",
    "X_r = rscaler.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fi = {}\n",
    "\n",
    "for k,v in ensembles.items():\n",
    "    fi[k] = v.fit(X_, y).feature_importances_\n",
    "\n",
    "fi = pd.DataFrame.from_dict(fi, orient='index', columns=X.columns)\n",
    "\n",
    "fi.sort_values(fi.columns.tolist(), ascending=False, inplace=True)\n",
    "fi.sort_values(fi.index.tolist(), axis=1, ascending=False, inplace=True)\n",
    "\n",
    "fi.style.highlight_max(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fi = {}\n",
    "\n",
    "for k,v in ensembles.items():\n",
    "    fi[k] = v.fit(X_r, y).feature_importances_\n",
    "\n",
    "fi = pd.DataFrame.from_dict(fi, orient='index', columns=X.columns)\n",
    "\n",
    "fi.sort_values(fi.columns.tolist(), ascending=False, inplace=True)\n",
    "fi.sort_values(fi.index.tolist(), axis=1, ascending=False, inplace=True)\n",
    "\n",
    "fi.style.highlight_max(axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The most important feature from all three ensemble algorithms is `hair_length`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EDA: `hair_length`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.type.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ = pd.DataFrame(X_, columns=X.columns).join(y)\n",
    "train_r = pd.DataFrame(X_r, columns=X.columns).join(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(train_, hue='type', x_vars=[\n",
    "    'hair_length',\n",
    "    'has_soul',\n",
    "], y_vars=[\n",
    "    'hair_length',\n",
    "    'has_soul',\n",
    "], height=4, aspect=1.5);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Interactions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define poly function for interactions\n",
    "poly = PolynomialFeatures(degree=3, interaction_only=False, include_bias=False)\n",
    "\n",
    "# create interaction and cubic terms, including bias\n",
    "poly_X = poly.fit_transform(X_)\n",
    "\n",
    "# get column names from poly\n",
    "poly_columns = poly.get_feature_names(X.columns)\n",
    "\n",
    "fi = {}\n",
    "\n",
    "for k,v in ensembles.items():\n",
    "    fi[k] = v.fit(poly_X, y).feature_importances_\n",
    "\n",
    "fi = pd.DataFrame.from_dict(fi, orient='index', columns=poly_columns)\n",
    "\n",
    "fi.sort_values(fi.columns.tolist(), ascending=False, inplace=True)\n",
    "fi.sort_values(fi.index.tolist(), axis=1, ascending=False, inplace=True)\n",
    "\n",
    "fi.style.highlight_max(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define poly function for interactions\n",
    "poly = PolynomialFeatures(degree=3, interaction_only=False, include_bias=False)\n",
    "\n",
    "# create interaction and cubic terms, including bias\n",
    "poly_X = poly.fit_transform(X_r)\n",
    "\n",
    "# get column names from poly\n",
    "poly_columns = poly.get_feature_names(X.columns)\n",
    "\n",
    "fi = {}\n",
    "\n",
    "for k,v in ensembles.items():\n",
    "    fi[k] = v.fit(poly_X, y).feature_importances_\n",
    "\n",
    "fi = pd.DataFrame.from_dict(fi, orient='index', columns=poly_columns)\n",
    "\n",
    "fi.sort_values(fi.columns.tolist(), ascending=False, inplace=True)\n",
    "fi.sort_values(fi.index.tolist(), axis=1, ascending=False, inplace=True)\n",
    "\n",
    "fi.style.highlight_max(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "poly_train = pd.DataFrame(poly_X, columns=poly_columns).join(y)\n",
    "\n",
    "g = sns.pairplot(poly_train, hue='type', x_vars=[\n",
    "    'hair_length^3',\n",
    "    'has_soul^3',\n",
    "    'hair_length',\n",
    "], y_vars=[\n",
    "    'hair_length^3',\n",
    "    'has_soul^3',\n",
    "    'hair_length',\n",
    "], height=4, aspect=1)\n",
    "\n",
    "g.map_offdiag(sns.kdeplot);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(16,8))\n",
    "\n",
    "poly_train.loc[\n",
    "    poly_train.type == 'Ghost',\n",
    "    'hair_length',\n",
    "].plot.kde(ax=ax, label='Ghost')\n",
    "\n",
    "poly_train.loc[\n",
    "    poly_train.type == 'Ghoul',\n",
    "    'hair_length',\n",
    "].plot.kde(ax=ax, label='Ghoul')\n",
    "\n",
    "poly_train.loc[\n",
    "    poly_train.type == 'Goblin',\n",
    "    'hair_length',\n",
    "].plot.kde(ax=ax, label='Goblin')\n",
    "\n",
    "ax.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After using the `RobustScaler` and adding adding cubic interactions it can be seen that **Ghost**s' tend to have `hair_length` $\\le 0$ and **Ghoul**s' have `hair_length` $\\ge 0$.\n",
    "\n",
    "**Goblin**s appear split $50$-$50$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# flag if hair length < 0 (aka < median)\n",
    "train['hair_length_lt0'] = (train.hair_length < train.hair_length.median()).astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Importance Part II"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select X features\n",
    "X = train.drop(columns=[\n",
    "    'id',\n",
    "    'color',\n",
    "    'type',\n",
    "])\n",
    "\n",
    "# select y for training\n",
    "y = train.type\n",
    "\n",
    "# define scaler\n",
    "scaler = MinMaxScaler()\n",
    "rscaler = RobustScaler()\n",
    "\n",
    "# scale X\n",
    "X_ = scaler.fit_transform(X)\n",
    "X_r = rscaler.fit_transform(X)\n",
    "\n",
    "# define poly function for interactions\n",
    "poly = PolynomialFeatures(degree=3, interaction_only=False, include_bias=False)\n",
    "\n",
    "# create interaction and cubic terms, including bias\n",
    "poly_X = poly.fit_transform(X_r)\n",
    "\n",
    "# get column names from poly\n",
    "poly_columns = poly.get_feature_names(X.columns)\n",
    "\n",
    "fi = {}\n",
    "\n",
    "for k,v in ensembles.items():\n",
    "    fi[k] = v.fit(poly_X, y).feature_importances_\n",
    "\n",
    "fi = pd.DataFrame.from_dict(fi, orient='index', columns=poly_columns)\n",
    "\n",
    "fi.sort_values(fi.columns.tolist(), ascending=False, inplace=True)\n",
    "fi.sort_values(fi.index.tolist(), axis=1, ascending=False, inplace=True)\n",
    "\n",
    "fi.style.highlight_max(axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`has_soul` still appears important by itself even after cubic interactions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EDA: `has_soul`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "poly_train = pd.DataFrame(poly_X, columns=poly_columns).join(y)\n",
    "\n",
    "g = sns.pairplot(poly_train, hue='type', x_vars=[\n",
    "    'has_soul',\n",
    "    'bone_length hair_length has_soul',\n",
    "], y_vars=[\n",
    "    'has_soul',\n",
    "    'bone_length hair_length has_soul',\n",
    "], height=4, aspect=1)\n",
    "\n",
    "g.map_offdiag(sns.kdeplot);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seems to be a similar situation for `has_soul`:\n",
    "- **Ghost**s are $\\le 0$\n",
    "- **Ghoul**s are $\\ge 0$\n",
    "- **Goblin**s are split $50$-$50$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# flag if has soul < 0 (aka < median)\n",
    "train['has_soul_lt0'] = (train.has_soul < train.has_soul.median()).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# counts by combination of hair_length_lt0 and has_soul_lt0\n",
    "train.groupby([\n",
    "    'hair_length_lt0',\n",
    "    'has_soul_lt0',\n",
    "    'type',\n",
    "]).id.count().unstack()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Importance: Part III"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select X features\n",
    "X = train.drop(columns=[\n",
    "    'id',\n",
    "    'color',\n",
    "    'type',\n",
    "])\n",
    "\n",
    "# select y for training\n",
    "y = train.type\n",
    "\n",
    "# define scaler\n",
    "scaler = MinMaxScaler()\n",
    "rscaler = RobustScaler()\n",
    "\n",
    "# scale X\n",
    "X_ = scaler.fit_transform(X)\n",
    "X_r = rscaler.fit_transform(X)\n",
    "\n",
    "# define poly function for interactions\n",
    "poly = PolynomialFeatures(degree=3, interaction_only=True, include_bias=False)\n",
    "\n",
    "# create interaction and cubic terms, including bias\n",
    "poly_X = poly.fit_transform(X_r)\n",
    "\n",
    "# get column names from poly\n",
    "poly_columns = poly.get_feature_names(X.columns)\n",
    "\n",
    "fi = {}\n",
    "\n",
    "for k,v in ensembles.items():\n",
    "    fi[k] = v.fit(poly_X, y).feature_importances_\n",
    "\n",
    "fi = pd.DataFrame.from_dict(fi, orient='index', columns=poly_columns)\n",
    "\n",
    "fi.sort_values(fi.columns.tolist(), ascending=False, inplace=True)\n",
    "fi.sort_values(fi.index.tolist(), axis=1, ascending=False, inplace=True)\n",
    "\n",
    "fi.style.highlight_max(axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After adding the two flags, `bone_length` has shown some promise with quadratic and cubic interactions (when limiting to interaction only)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EDA: `bone_length`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "poly_train = pd.DataFrame(poly_X, columns=poly_columns).join(y)\n",
    "\n",
    "g = sns.pairplot(poly_train, hue='type', x_vars=[\n",
    "    'bone_length',\n",
    "    'hair_length',\n",
    "], y_vars=[\n",
    "    'bone_length',\n",
    "    'hair_length',\n",
    "], height=4, aspect=1)\n",
    "\n",
    "g.map_offdiag(sns.kdeplot);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar situation:\n",
    "- **Ghost**s `bone_length` $\\le 0$\n",
    "- **Ghoul**s `bone_length` $\\ge 0$\n",
    "- **Goblin**s split $50$-$50$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# flag if bone length < 0 (aka < median)\n",
    "train['bone_length_lt0'] = (train.bone_length < train.bone_length.median()).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# counts by combination of hair_length_lt0, has_soul_lt0, and bone_length_lt0\n",
    "train.groupby([\n",
    "    'hair_length_lt0',\n",
    "    'has_soul_lt0',\n",
    "    'bone_length_lt0',\n",
    "    'type',\n",
    "]).id.count().unstack()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Ghoul**s tend to be above the median on `hair_length`, `has_soul`, and `bone_length`.\n",
    "- **Ghost**s tend to be below the median on `hair_length`, `has_soul`, and `bone_length`.\n",
    "- **Goblin**s tend to be at the extremes (all above or all below)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shouldn't sum features like this as the model accounts for them better at an individual level\n",
    "\n",
    "# # sum hair_length_lt0, has_soul_lt0, bone_length_lt0 flags\n",
    "# train['lt0_sum_hair_soul_bone'] = train.hair_length_lt0 + train.has_soul_lt0 + train.bone_length_lt0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # counts by lt0_sum_hair_soul_bone\n",
    "# train.groupby([\n",
    "#     'lt0_sum_hair_soul_bone',\n",
    "#     'type',\n",
    "# ]).id.count().unstack()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Importance Part IV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select X features\n",
    "X = train.drop(columns=[\n",
    "    'id',\n",
    "    'color',\n",
    "    'type',\n",
    "])\n",
    "\n",
    "# select y for training\n",
    "y = train.type\n",
    "\n",
    "# define scaler\n",
    "scaler = MinMaxScaler()\n",
    "rscaler = RobustScaler()\n",
    "\n",
    "# scale X\n",
    "X_ = scaler.fit_transform(X)\n",
    "X_r = rscaler.fit_transform(X)\n",
    "\n",
    "# define poly function for interactions\n",
    "poly = PolynomialFeatures(degree=2, interaction_only=False, include_bias=False)\n",
    "\n",
    "# create interaction and cubic terms, including bias\n",
    "poly_X = poly.fit_transform(X_r)\n",
    "\n",
    "# get column names from poly\n",
    "poly_columns = poly.get_feature_names(X.columns)\n",
    "\n",
    "fi = {}\n",
    "\n",
    "for k,v in ensembles.items():\n",
    "    fi[k] = v.fit(poly_X, y).feature_importances_\n",
    "\n",
    "fi = pd.DataFrame.from_dict(fi, orient='index', columns=poly_columns)\n",
    "\n",
    "fi.sort_values(fi.columns.tolist(), ascending=False, inplace=True)\n",
    "fi.sort_values(fi.index.tolist(), axis=1, ascending=False, inplace=True)\n",
    "\n",
    "fi.style.highlight_max(axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`rotting_flesh` has popped up in a few places:\n",
    "\n",
    "- ~~`rotting_flesh^2 lt0_sum_hair_soul_bone` with cubic interaction~~\n",
    "- `rotting_flesh` with quadratic interaction\n",
    "- `rotting_flesh` with no interaction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EDA: `rotting_flesh`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "poly_train = pd.DataFrame(poly_X, columns=poly_columns).join(y)\n",
    "\n",
    "g = sns.pairplot(poly_train, hue='type', vars=[\n",
    "    'rotting_flesh',\n",
    "    'hair_length',\n",
    "], height=4, aspect=1)\n",
    "\n",
    "g.map_offdiag(sns.kdeplot);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# flag if rotting flesh < 0 (aka < median)\n",
    "train['rotting_flesh_lt0'] = (train.rotting_flesh < train.rotting_flesh.median()).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# counts by combination of hair_length_lt0, has_soul_lt0, bone_length_lt0, rotting_flesh_lt0\n",
    "train.groupby([\n",
    "    'hair_length_lt0',\n",
    "    'has_soul_lt0',\n",
    "    'bone_length_lt0',\n",
    "    'rotting_flesh_lt0',\n",
    "    'type',\n",
    "]).id.count().unstack()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this rate, there could be some kind of interaction effect between all of my `lt0` features.\n",
    "\n",
    "I have $4$ of them, so let's use 4-way interaction terms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# select X features\n",
    "X = train.drop(columns=[\n",
    "    'id',\n",
    "    'color',\n",
    "    'type',\n",
    "])\n",
    "\n",
    "# select y for training\n",
    "y = train.type\n",
    "\n",
    "# define scaler\n",
    "scaler = MinMaxScaler()\n",
    "rscaler = RobustScaler()\n",
    "\n",
    "# define vt\n",
    "vt = VarianceThreshold()\n",
    "\n",
    "# define selector\n",
    "selector = SelectPercentile()\n",
    "\n",
    "# scale X\n",
    "X_ = scaler.fit_transform(X)\n",
    "X_r = rscaler.fit_transform(X)\n",
    "\n",
    "# define poly function for interactions\n",
    "# include interaction only\n",
    "poly = PolynomialFeatures(degree=4, interaction_only=True, include_bias=False)\n",
    "\n",
    "# create interaction and cubic terms, including bias\n",
    "poly_X = poly.fit_transform(X_r)\n",
    "\n",
    "# remove fields with no variance\n",
    "poly_X = vt.fit_transform(poly_X)\n",
    "\n",
    "# keep top 10% of best fields\n",
    "poly_X = selector.fit_transform(poly_X, y)\n",
    "\n",
    "# get boolean support on vt fields\n",
    "vt_columns = vt.get_support()\n",
    "\n",
    "# get boolean support on selector fields\n",
    "selector_columns = selector.get_support()\n",
    "\n",
    "# get column names from poly\n",
    "poly_columns = np.array(poly.get_feature_names(X.columns))[vt_columns][selector_columns]\n",
    "\n",
    "fi = {}\n",
    "\n",
    "for k,v in ensembles.items():\n",
    "    fi[k] = v.fit(poly_X, y).feature_importances_\n",
    "\n",
    "fi = pd.DataFrame.from_dict(fi, orient='index', columns=poly_columns)\n",
    "\n",
    "fi.sort_values(fi.columns.tolist(), ascending=False, inplace=True)\n",
    "fi.sort_values(fi.index.tolist(), axis=1, ascending=False, inplace=True)\n",
    "\n",
    "fi.style.highlight_max(axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# put the poly data into a dataframe\n",
    "poly_df = pd.DataFrame(\n",
    "    data=poly_X,\n",
    "    index=X.index,\n",
    "    columns=poly_columns\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# heatmap of correlation\n",
    "fig, ax = plt.subplots(figsize=(16,8))\n",
    "\n",
    "sns.heatmap(poly_df.corr(), ax=ax);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store fields with \"high\" correlation (> 0.6)\n",
    "corr = {}\n",
    "\n",
    "for c in poly_df.columns:\n",
    "    corr[c] = poly_df.corr()[c].where(lambda x : x.abs() > 0.6).dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Principal Component Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define pca (retain 99% of explainable variance)\n",
    "pca = PCA(n_components=0.99)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_pca = {}\n",
    "\n",
    "for k,v in corr.items():\n",
    "    corr_pca[k] = pca.fit_transform(poly_df.loc[:, v.index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_v = set()\n",
    "\n",
    "for k,v in corr.items():\n",
    "    corr_v = corr_v.union(set([k]))\n",
    "    corr_v = corr_v.union(set(v.index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pca = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for k,v in corr_pca.items():\n",
    "    train_pca = pd.concat([\n",
    "        train_pca,\n",
    "        pd.DataFrame(v).add_prefix(f\"{k}_\")\n",
    "    ], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tpc = train_pca.corr()\n",
    "\n",
    "tpc.sort_values(tpc.columns.tolist(), inplace=True)\n",
    "\n",
    "tpc.sort_values(tpc.index.tolist(), axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(16,8))\n",
    "\n",
    "sns.heatmap(tpc, ax=ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pca = train_pca.join(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select X features\n",
    "X = train_pca.drop(columns=[\n",
    "#     'id',\n",
    "#     'color',\n",
    "    'type',\n",
    "])\n",
    "\n",
    "# select y for training\n",
    "y = train_pca.type\n",
    "\n",
    "# define scaler\n",
    "scaler = MinMaxScaler()\n",
    "rscaler = RobustScaler()\n",
    "\n",
    "# define vt\n",
    "vt = VarianceThreshold()\n",
    "\n",
    "# define selector\n",
    "selector = SelectPercentile()\n",
    "\n",
    "# scale X\n",
    "X_ = scaler.fit_transform(X)\n",
    "X_r = rscaler.fit_transform(X)\n",
    "\n",
    "# define poly function for interactions\n",
    "# include interaction only\n",
    "poly = PolynomialFeatures(degree=4, interaction_only=True, include_bias=False)\n",
    "\n",
    "# create interaction and cubic terms, including bias\n",
    "# poly_X = poly.fit_transform(X_r)\n",
    "poly_X = X_r\n",
    "\n",
    "# remove fields with no variance\n",
    "poly_X = vt.fit_transform(poly_X)\n",
    "\n",
    "# keep top 10% of best fields\n",
    "poly_X = selector.fit_transform(poly_X, y)\n",
    "\n",
    "# get boolean support on vt fields\n",
    "vt_columns = vt.get_support()\n",
    "\n",
    "# get boolean support on selector fields\n",
    "selector_columns = selector.get_support()\n",
    "\n",
    "# get column names from poly\n",
    "# poly_columns = np.array(poly.get_feature_names(X.columns))[vt_columns][selector_columns]\n",
    "poly_columns = np.array(X.columns)[vt_columns][selector_columns]\n",
    "\n",
    "fi = {}\n",
    "\n",
    "for k,v in ensembles.items():\n",
    "    fi[k] = v.fit(poly_X, y).feature_importances_\n",
    "\n",
    "fi = pd.DataFrame.from_dict(fi, orient='index', columns=poly_columns)\n",
    "\n",
    "fi.sort_values(fi.columns.tolist(), ascending=False, inplace=True)\n",
    "fi.sort_values(fi.index.tolist(), axis=1, ascending=False, inplace=True)\n",
    "\n",
    "fi.style.highlight_max(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = sns.pairplot(train_pca, hue='type', vars=[\n",
    "    'hair_length_lt0 has_soul_lt0_0',\n",
    "    'hair_length has_soul hair_length_lt0 has_soul_lt0_0',\n",
    "    'bone_length bone_length_lt0_0'\n",
    "], height=4, aspect=1.5)\n",
    "\n",
    "g.map_offdiag(sns.kdeplot);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x, test_x, train_y, test_y = train_test_split(train_pca.drop(columns=[\n",
    "    'type'\n",
    "]), train_pca.type, test_size=0.1, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "results = {}\n",
    "\n",
    "for k,v in classifiers.items():\n",
    "    \n",
    "    print(k)\n",
    "    \n",
    "    cv = cross_validate(\n",
    "        estimator=v,\n",
    "        X=train_x,\n",
    "        y=train_y,\n",
    "        cv=10\n",
    "    )\n",
    "    \n",
    "    cv = cv['test_score']\n",
    "    \n",
    "    results[k] = cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame.from_dict(results, orient='columns').describe()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
